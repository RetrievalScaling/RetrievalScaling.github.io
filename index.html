<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Scaling Retrieval-Based Language Models with a Trillion-Token Datastore">
  <meta name="keywords" content="scaling retrieval-based language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Scaling Retrieval-Based Language Models with a Trillion-Token Datastore</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Scaling Retrieval-Based Language Models with a Trillion-Token Datastore</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://rulinshao.github.io/">Rulin Shao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://jacqueline-he.github.io/">Jacqueline He</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://akariasai.github.io">Akari Asai</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://swj0419.github.io/">Weijia Shi</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://timdettmers.com/">Tim Dettmers</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.sewonmin.com/">Sewon Min</a><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://www.cs.washington.edu/people/faculty/lsz">Luke Zettlemoyer</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://koh.pw/">Pang Wei Koh</a><sup>12</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Washington,</span>
            <span class="author-block"><sup>2</sup>Allen Institute for AI</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2407.12854"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/RulinShao/retrieval-scaling"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/rulins/MassiveDS-1.4T"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="static/images/scaling_gif.gif" alt="Datastore Scaling.">
        <br>
        <br>
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. 
            In this paper, we consider another dimension of scaling: the amount of data available at <i>inference</i> time.
            Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, 
            such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks.
            By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget.
          </p>
          <p>
            We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in a computationally accessible manner.
            Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. 
            Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. 
            To facilitate future research, we open-source all artifacts.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">MassiveDS and our Datastore Scaling Pipeline</h2>
          <div class="content has-text-justified">
            <p>
            <b>MassiveDS</b> MassiveDS is constructed from diverse data sources including both domain-specific data and massive web data (Table 2), so that it can benefit a wide range of tasks.
            </p>
            <img src="static/images/massiveds.png" alt="MassiveDS overview." width="400" style="display: block; margin-left: auto; margin-right: auto;">
            <p>
            We also designed an efficient scaling experiment pipeline (Figure 2) that lets us understand scaling trends on an academic budget. The key idea is to reorder the operations to make the repeated ones cheaper. We prove its equivalence to the naive implementation in Appendix A.
            </p>
            <img src="static/images/pipeline.png" alt="Pipeline overview.">
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </section>
  </div>
    <!--/ Results and Analysis. -->
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Scaling Trends</h2>
          <div class="content has-text-justified">
            <h4 class="title is-4">Datastore Scaling</h4>
            <p>
              Previous work has shown scaling datastore is helpful for language modeling, while it remains unknown how it applies to downstream tasks. We show MassiveDS not only helps language modeling but also downstream tasks including MMLU.
            </p>
            <img src="static/images/datastore_scaling_results.png" alt="Datastore Scaling overview.">
          </div>
          <div class="content has-text-justified">
            <h4 class="title is-4">Compute-Optimal Scaling</h4>
            <p>
              We find using larger datastores can significantly improve performance for the same training compute. Figure 4 shows compute-optimal scaling curves with OLMo and Pythia on 4 downstream tasks.
            </p>
            <img src="static/images/compute_optimal_scaling_results.png" alt="Compute-Optimal Scaling overview.">
          </div>
          <h2 class="title is-3">Analysis</h2>
          <div class="content has-text-justified">
            <h4 class="title is-4">Retriever is Robust to Out-of-Distribution Data in the Datastore</h4>
            <p>
              We compare the performance of MassiveDS with single-domain datastores in Table 3. The results show that MassiveDS matches or outperforms all single-domain datastores.
            </p>
            <img src="static/images/single_comain_comparison.png" alt="Single Domain overview.">
            <p>
              We further investigate the source of this robustness and find that the retriever tends to retrieve more from the relevant domain depite the the existance of a large amout of out-of-distribution data, as shown in Figure 5. 
            </p>
            <img src="static/images/retrieval_distribution.png" alt="Retrieval Distribution overview." width="400" style="display: block; margin-left: auto; margin-right: auto;">
            <p>
              Overall, these results show that retrieving from broad datastores like MassiveDS can simultaneously improve performance across multiple domains, paving the path towards general-purpose retrieval-based models.
            </p>
          </div>
          <div class="content has-text-justified">
            <h4 class="title is-4">Impact of Reranker and Data Contamination</h4>
            <p>
              As shown in Figure 6, we find scaling trends can be improved with advanced retrieval, such as reranking. In addition, we find data contamination has a large impact on language modeling evaluation, as shown in Figure 7, so we advocate applying strict decontamination for RAG perplexity evaluation.
            </p>
            <img src="static/images/analysis.png" alt="Analysis overview.">
          </div>
        </div>
      </div>
      <!--/ Future Directions. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Future Directions</h2>
          <div class="content has-text-justified">
            <p>
              Our compute-optimal scaling trends indicate that retrieval-based language models (LMs) scale better than standalone LMs. 
              Based on these findings, future research could focus on designing more effective training resource allocation strategies for retrieval-based LMs.
            </p>
            <p>
              We demonstrate that storing trillion-token data in a datastore can effectively enhance language modeling and several downstream tasks. 
              This approach introduces new challenges in serving retrieval-based language models (LMs) and highlights the need for efficient serving of large-scale datastores coupled with LMs. 
              Future work could utilize MassiveDS to test new efficient index structures and systems.
            </p>
            <p>
              There has been ongoing discussion comparing retrieval-based LMs to long-context LMs. 
              Our work relates to this in that we retrieve data from trillions of tokens and prepend it to the context, which can be seen as a method to achieve a 1-trillion-token context length through a sparse context selection process. 
              On the other hand, using a long-context LM enables us to include more retrieved tokens within the context. 
              We are curious about potential follow-ups in this direction.
            </p>
          </div>
        </div>
      </div>
      <!--/ Code and Data. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Reprduction and Datastore</h2>
          <div class="content has-text-justified">
            <p>
            All code is available at <a href="https://github.com/RulinShao/retrieval-scaling">Github</a>, and all datastore artifacts are available at <a href="https://huggingface.co/datasets/rulins/MassiveDS-1.4T">Huggingface Space</a>.
            </p>
          </div>
        </div>
      </div>
    </section>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{shao2024scaling,
  author = {Shao, Rulin and He, Jacqueline and Asai, Akari and Shi, Weijia and Dettmers, Tim and Min, Sewon and Zettlemoyer, Luke and Koh, Pang Wei},
  title  = {Scaling Retrieval-Based Language Models with a Trillion-Token Datastore},
  year   = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2407.12854">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/RulinShao/retrieval-scaling" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Template adopted from the <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies project</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
